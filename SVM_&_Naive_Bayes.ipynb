{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Support Vector Machine (SVM), and how does it work?\n",
        "- A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. For classification, SVM finds a decision boundary (hyperplane) that best separates classes by maximizing the margin — the distance between the hyperplane and the nearest data points from each class (these nearest points are called support vectors).\n",
        "2. Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "- Hard Margin SVM: assumes data are perfectly linearly separable. It enforces zero training error and maximizes margin subject to no misclassification. Because it requires perfect separability, it’s sensitive to outliers and noise.\n",
        "\n",
        "- Soft Margin SVM: allows some misclassifications by introducing slack variables (ξᵢ ≥ 0). The objective becomes maximizing margin while penalizing misclassification errors weighted by C. Smaller C → wider margin, more tolerance for misclassification; larger C → fewer misclassifications, potentially narrower margin. Soft margin is used in practice because data often are noisy/not perfectly separable.\n",
        "3. What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.\n",
        "- The Kernel Trick lets SVM compute dot products in a high-dimensional (possibly infinite-dimensional) feature space without explicitly mapping data into that space. Instead, a kernel function K(x, x') = φ(x) · φ(x') computes the inner product of transformed features φ(x). This allows SVM to find nonlinear decision boundaries efficiently.\n",
        "\n",
        "- Common kernels:\n",
        "\n",
        "  - Linear kernel: K(x,x') = x·x' — use when data are already linearly separable or features are high-dimensional and linear decision is appropriate.\n",
        "\n",
        "  - Polynomial kernel: K(x,x') = (γ x·x' + r)^d — useful when interactions up to degree d matter.\n",
        "\n",
        "  - Radial Basis Function (RBF, Gaussian) kernel: K(x,x') = exp(-γ ||x - x'||²) — widely used; handles many nonlinear relationships; has parameter γ controlling kernel width.\n",
        "\n",
        "- Example use case: RBF kernel for classification of nonlinearly separable data such as complex biological measurements where class boundary is curved in original feature space.\n",
        "4. What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "- A Naïve Bayes classifier is a probabilistic classifier based on Bayes’ theorem. It models the posterior probability P(Class | Features) proportional to P(Class) * P(Features | Class). The key simplification is the naïve assumption: features are conditionally independent given the class. This reduces computation: P(x1, x2, ..., xn | class) = ∏ P(xi | class). Despite the strong (often false) independence assumption, Naïve Bayes performs well in many domains, especially text classification.\n",
        "\n",
        "- Advantages:\n",
        "\n",
        "  - Fast to train and predict\n",
        "\n",
        "  - Works well with high-dimensional inputs (e.g., bag-of-words)\n",
        "\n",
        "  - Requires relatively little training data\n",
        "\n",
        "- Limitations:\n",
        "\n",
        "  - The conditional independence assumption may be unrealistic; correlated features can hurt performance.\n",
        "\n",
        "  - Continuous features need modeling (e.g., Gaussian assumption) or discretization.\n",
        "  5. Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.\n",
        "When would you use each one?\n",
        "- Gaussian Naïve Bayes: Assumes continuous features have a Gaussian (normal) distribution for each class. Use when features are continuous real-valued (e.g., height, weight, sensor measures) and roughly normally distributed. Implementation computes class-wise mean and variance.\n",
        "\n",
        "- Multinomial Naïve Bayes: Models counts or frequency features where data represent counts per feature (e.g., word counts in documents). Typical for document classification with bag-of-words or TF counts. Works with integer count vectors or normalized frequencies.\n",
        "\n",
        "- Bernoulli Naïve Bayes: Models binary occurrence features (0/1) indicating presence/absence of a feature (e.g., whether a word appears in a document). Use when features are binary indicators rather than counts.\n",
        "\n",
        "- Rule of thumb:\n",
        "\n",
        "  - Text classification with word counts → Multinomial NB.\n",
        "\n",
        "  - Text classification with binary word indicators → Bernoulli NB.\n",
        "\n",
        "  - Continuous numeric features → Gaussian NB."
      ],
      "metadata": {
        "id": "qo_5NFZ4_ACg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python program to:\n",
        "# ● Load the Iris dataset\n",
        "# ● Train an SVM Classifier with a linear kernel\n",
        "# ● Print the model's accuracy and support vectors.\n",
        "# Question 6 - SVM on Iris\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "clf = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy on test set: {acc:.4f}\")\n",
        "\n",
        "print(\"Number of support vectors for each class:\", clf.n_support_)\n",
        "print(\"Support vectors array shape:\", clf.support_vectors_.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCGWJUfVE8hs",
        "outputId": "8b694380-dc21-48e0-8797-fb775d0922ef"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 1.0000\n",
            "Number of support vectors for each class: [ 3 10  9]\n",
            "Support vectors array shape: (22, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Write a Python program to:\n",
        "# ● Load the Breast Cancer dataset\n",
        "# ● Train a Gaussian Naïve Bayes model\n",
        "# ● Print its classification report including precision, recall, and F1-score.\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "data = datasets.load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hFsAaUeFSZv",
        "outputId": "3e78baea-fe72-4f9f-ff7f-87ef8234034f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9370629370629371\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.96      0.87      0.91        53\n",
            "      benign       0.93      0.98      0.95        90\n",
            "\n",
            "    accuracy                           0.94       143\n",
            "   macro avg       0.94      0.92      0.93       143\n",
            "weighted avg       0.94      0.94      0.94       143\n",
            "\n",
            "Confusion Matrix:\n",
            " [[46  7]\n",
            " [ 2 88]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python program to:\n",
        "# ● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "# C and gamma.\n",
        "# ● Print the best hyperparameters and accuracy.\n",
        "# Question 8 - Grid search for SVM hyperparameters on Wine dataset\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': ['scale', 'auto', 0.01, 0.1, 1],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "svc = SVC()\n",
        "grid = GridSearchCV(svc, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best hyperparameters:\", grid.best_params_)\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(\"Test accuracy with best params:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaQE4QIMFnhY",
        "outputId": "70e849cf-6d24-414d-ccd8-72de49479666"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best hyperparameters: {'C': 100, 'gamma': 'scale', 'kernel': 'rbf'}\n",
            "Test accuracy with best params: 0.8222222222222222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python program to:\n",
        "# ● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "# sklearn.datasets.fetch_20newsgroups).\n",
        "# ● Print the model's ROC-AUC score for its predictions.\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
        "\n",
        "categories = ['sci.space', 'rec.sport.hockey']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "vect = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
        "X_train_tfidf = vect.fit_transform(X_train)\n",
        "X_test_tfidf = vect.transform(X_test)\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_proba = clf.predict_proba(X_test_tfidf)[:, 1]\n",
        "y_pred = clf.predict(X_test_tfidf)\n",
        "\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"ROC-AUC score: {auc:.4f}\\n\")\n",
        "\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WclYrJ5F3vC",
        "outputId": "29ca5768-5cf6-49c5-ed53-b182b5bf4a9d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC score: 0.9957\n",
            "\n",
            "Classification report:\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "rec.sport.hockey       0.93      0.98      0.96       300\n",
            "       sci.space       0.98      0.93      0.95       296\n",
            "\n",
            "        accuracy                           0.95       596\n",
            "       macro avg       0.96      0.95      0.95       596\n",
            "    weighted avg       0.96      0.95      0.95       596\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "● Text with diverse vocabulary\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "● Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "● Address class imbalance\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "-\n",
        "\n",
        "```\n",
        "# Sample spam-detection pipeline (MultinomialNB baseline)\n",
        "# Note: Replace `emails` & `labels` with your dataset (emails: list of text, labels: 0=ham,1=spam)\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, roc_auc_score, average_precision_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "\n",
        "# Example placeholder data - replace with real dataset\n",
        "emails = [\n",
        "    \"Win a free phone now! Click here to claim your prize\",\n",
        "    \"Monthly invoice attached, please review\",\n",
        "    \"Limited time offer: cheap meds, no prescription\",\n",
        "    \"Meeting agenda for tomorrow attached\"\n",
        "]\n",
        "labels = [1, 0, 1, 0]  # 1=spam, 0=not spam\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(emails, labels, test_size=0.25, random_state=42, stratify=labels)\n",
        "\n",
        "# Pipeline: TF-IDF + MultinomialNB\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words='english', max_df=0.8, ngram_range=(1,2))),\n",
        "    ('clf', MultinomialNB())\n",
        "])\n",
        "\n",
        "# Optionally tune alpha using GridSearch\n",
        "param_grid = {'clf__alpha': [0.1, 0.5, 1.0]}\n",
        "grid = GridSearchCV(pipeline, param_grid, cv=3, scoring='f1', n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best params:\", grid.best_params_)\n",
        "y_pred = grid.predict(X_test)\n",
        "y_proba = grid.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Classification report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "# Use PR-AUC for imbalanced data\n",
        "print(\"Average Precision (PR-AUC):\", average_precision_score(y_test, y_proba))\n",
        "# ROC-AUC (works but PR-AUC better for imbalance)\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba))\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FokxcJnbGdpn"
      }
    }
  ]
}